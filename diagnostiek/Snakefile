"""
AMC pipeline to process MiSeq data for diagnostics.
Martin Haagmans https://github.com/zaag
February 2017
"""
import os
import csv
import sys
import glob
import gzip
import shutil
import sqlite3
import subprocess

import vcf
import xlsxwriter
import matplotlib
import numpy as np
import pandas as pd

from ngsscriptlibrary.parsing import *

from ngsscriptlibrary import sample_in_db
from ngsscriptlibrary import metrics2db
from ngsscriptlibrary import perc_target_covered2db
from ngsscriptlibrary import summary2db

matplotlib.use('Agg')

# Define variables from config file
GATK = config["GATK"]
REF = config["REF"]
DBSNP = config["DBSNP"]
PICARD = config["PICARD"]
TARGETREPO = config["TARGET"]
DBREPO = config['DB']
JAVA = config['JAVA']
pyCNV = config['pyCNV']

# Define files
TARGETDB = os.path.join(TARGETREPO, 'varia', 'captures.sqlite')
METRICSDB = os.path.join(DBREPO, 'metrics.sqlite')
SOMOSADB = os.path.join(DBREPO, 'SO.mosaic.sqlite')
SNPCHECK = os.path.join(TARGETREPO, 'varia', 'NGS-SNPcheck.vcf')
samplesheet = 'SampleSheet.csv'


def get_file_locations(todo, targetrepo):
    """Read dict with targets and analyses and add correct file locations.
    Return dict
    """
    for s in todo.keys():
        picard = '{}_target.interval_list'.format(todo[s]['capture'])
        picard = os.path.join(targetrepo, 'captures', picard)

        cnvtarget = '{}_target.bed'.format(todo[s]['capture'])
        cnvtarget = os.path.join(targetrepo, 'captures', cnvtarget)

        annot = '{}_target.annotated'.format(todo[s]['capture'])
        annot = os.path.join(targetrepo, 'captures', annot)

        cap_is_pakket = todo[s]['capispakket']

        if cap_is_pakket:
            varcal = '{}_generegions.bed'.format(todo[s]['capture'])
            varcal = os.path.join(targetrepo, 'captures', varcal)
            sanger = '{}_target.bed'.format(todo[s]['capture'])
            sanger = os.path.join(targetrepo, 'captures', sanger)
            pakket = '{}_target.bed'.format(todo[s]['capture'])
            pakket = os.path.join(targetrepo, 'captures', pakket)
        elif not cap_is_pakket:
            varcal = '{}_generegions.bed'.format(todo[s]['pakket'])
            varcal = os.path.join(targetrepo, 'pakketten', varcal)
            sanger = '{}_target.bed'.format(todo[s]['pakket'])
            sanger = os.path.join(targetrepo, 'pakketten', sanger)
            pakket = '{}_target.bed'.format(todo[s]['pakket'])
            pakket = os.path.join(targetrepo, 'pakketten', pakket)

        if todo[s]['panel'] is not None:
            sanger = '{}_target.bed'.format(todo[s]['panel'])
            sanger = os.path.join(targetrepo, 'panels', sanger)

        todo[s]['annot'] = annot
        todo[s]['picard'] = picard
        todo[s]['cnvtarget'] = cnvtarget
        todo[s]['pakkettarget'] = pakket
        todo[s]['varcal'] = varcal
        todo[s]['sanger'] = sanger

    return todo


def get_pakketten(todo):
    pakketten = list()
    for s in todo.keys():
        pakket = todo[s]['pakket']
        if pakket not in pakketten:
            pakketten.append(pakket)
    return pakketten


def get_captures(todo):
    captures = list()
    for s in todo.keys():
        capture = todo[s]['capture']
        if capture not in captures:
            captures.append(capture)
    return captures


def create_recalinput_file(samples):
    """Create a file with a list of BAM-files (1 per sample) to use as input
    for the recalibration step in the pipeline.
    """
    with open('recalinput.list', 'w') as f:
        for sample in samples:
            f.write('output/{}.DM.bam\n'.format(sample))


# Get todo list for samples
input_dict = parse_samplesheet_for_pipeline(samplesheet, TARGETDB)
input_dict = get_file_locations(input_dict, TARGETREPO)
captures = get_captures(input_dict)
pakketten = get_pakketten(input_dict)
samples = input_dict.keys()
serie = [input_dict[_]['serie'] for _ in input_dict.keys()][0]


# Rules for snakemake
onsuccess:
    shell("if [ -f recalinput.list ]; then rm -f recalinput.list ; fi ;")
    shell("if [ -d picardmetrics ]; then rmdir picardmetrics ; fi ;")
    shell("if [ -d gatkfiles ]; then rmdir gatkfiles ; fi ;")
    shell("if [ -d alignments ]; then rmdir alignments ; fi ;")


rule prepare:
    input:
        "SampleSheet.csv"
    output:
        expand("reads/{sample}.R1.fastq.gz", sample=samples),
        expand("reads/{sample}.R2.fastq.gz", sample=samples)
    run:
        for sample in samples:
            R1 = glob.glob('{s}*R1*.gz'.format(s=sample))
            R2 = glob.glob('{s}*R2*.gz'.format(s=sample))
            if R1 and R2:
                os.rename(R1[0], 'reads/{}.R1.fastq.gz'.format(sample))
                os.rename(R2[0], 'reads/{}.R2.fastq.gz'.format(sample))


rule countbases:
    input:
        expand("reads/{sample}.R1.fastq.gz", sample=samples)
    output:
        temp("gatkfiles/BasePercentages.txt")
    message:
        "Calculating per base percentage for each R1"
    run:
        get_base_count_filelist(output[0], input)


rule mapreads:
    input:
        "reads/{sample}.R1.fastq.gz",
        "reads/{sample}.R2.fastq.gz"
    output:
        temp("alignments/{sample}.sorted.bam")
    threads:
        2
    log:
        "logfiles/{sample}.BWAlignment.log"
    message:
        "Aligning reads with bwa mem"
    params:
        rg = "@RG\\tID:{sample}\\tLB:{sample}\\tPL:ILLUMINA\\tPU:{sample}\\tSM:{sample}"
    shell:
        '''(bwa mem  -R '{params.rg}' -t 1 -M {REF} {input} |\
        samtools view -Shu - |\
        samtools sort -T {wildcards.sample}.tmp -O bam - > {output}) > {log}  2>&1
        '''


rule markduplicates:
    input:
        rules.mapreads.output
    output:
        bam = "output/{sample}.DM.bam",
        metrics = temp("picardmetrics/{sample}.dupmark.txt")
    log:
        "logfiles/{sample}.MarkDuplicates.log"
    message:
        "Marking duplicates with picard"
    shell:
        '''{PICARD} MarkDuplicates  I={input} O={output.bam}  \
        M={output.metrics} REMOVE_DUPLICATES=FALSE CREATE_INDEX=true > {log}  2>&1
        '''


rule hsmetrics:
    input:
        rules.markduplicates.output.bam
    output:
        temp("picardmetrics/{sample}.HSMetrics.txt"),
    log:
        "logfiles/{sample}.HSmetrics.log"
    message:
        "Calculating HS-metrics with picard"
    run:
        target = input_dict[wildcards.sample]['picard']
        serie = input_dict[wildcards.sample]['serie']
        if sample_in_db(wildcards.sample, serie, METRICSDB, 'hsmetrics'):
            shell("touch {output}")

        elif not sample_in_db(wildcards.sample, serie, METRICSDB, 'hsmetrics'):
            shell('''{PICARD} CalculateHsMetrics R={REF} I={input} \
            O={output[0]} TI={target} BI={target} > {log} 2>&1
            ''')
            metrics = readmetrics(wildcards.sample, serie, output[0])
            metrics2db(metrics, METRICSDB, 'hsmetrics')


rule insertsizemetrics:
    input:
        rules.markduplicates.output.bam
    output:
        tabel = temp("picardmetrics/{sample}.InsertSize.txt"),
        pdf = temp("picardmetrics/{sample}.InsertSize.pdf")
    log:
        "logfiles/{sample}.InsertSize.log"
    message:
        "Calculating insertsize metrics"
    run:
        serie = input_dict[wildcards.sample]['serie']
        if sample_in_db(wildcards.sample, serie, METRICSDB, 'insertsize'):
            shell("touch {output.pdf}")
            shell("touch {output.tabel}")
        elif not sample_in_db(wildcards.sample, serie, METRICSDB, 'insertsize'):
            shell('''{PICARD} CollectInsertSizeMetrics R={REF} I={input} \
            O={output.tabel} HISTOGRAM_FILE={output.pdf} > {log} 2>&1
            ''')
            metrics = readmetrics(wildcards.sample, serie, output.tabel)
            metrics2db(metrics, METRICSDB, 'insertsize')


rule alignmetrics:
    input:
        rules.markduplicates.output.bam
    output:
        temp("picardmetrics/{sample}.AlignmentMetrics.txt")
    log:
        "logfiles/{sample}.AlignmentMetrics.log"
    message:
        "Calculating alignment metrics"
    run:
        serie = input_dict[wildcards.sample]['serie']

        if sample_in_db(wildcards.sample, serie, METRICSDB, 'alignment'):
            shell("touch {output}")
        elif not sample_in_db(wildcards.sample, serie, METRICSDB, 'alignment'):
            shell('''{PICARD} CollectAlignmentSummaryMetrics R={REF} I={input} \
            O={output[0]}  MAX_INSERT_SIZE=500 > {log} 2>&1
            ''')
            serie = input_dict[wildcards.sample]['serie']
            metrics = readmetrics(wildcards.sample, serie, output[0])
            metrics2db(metrics, METRICSDB, 'alignment')


rule recalibrate:
    input:
        expand("output/{sample}.DM.bam", sample=samples)
    output:
        "reads/bqsr.grp"
    log:
        "logfiles/Recal.log"
    message:
        "Recalibrating quality scores with GATK"
    threads:
        10
    run:
        create_recalinput_file(samples)
        shell('''{JAVA} -Xmx10g -jar {GATK} -R {REF} -T BaseRecalibrator -knownSites {DBSNP} \
        -I recalinput.list -o {output} -nct {threads} > {log} 2>&1
        ''')


rule callableloci:
    input:
        bam = rules.markduplicates.output.bam,
        table = rules.recalibrate.output
    output:
        bed = temp("gatkfiles/{sample}.bed"),
        summary = temp("gatkfiles/{sample}.summary")
    message:
        "Defining callable regions with GATK"
    log:
        "logfiles/{sample}.CallableLoci.log"
    run:
        target = input_dict[wildcards.sample]['sanger']

        shell('''{JAVA} -jar {GATK} -R {REF} -T CallableLoci \
        -minDepth 30 -mdflmq 30 -mmq 20 -frlmq 0.6 -mlmq 10 \
        -I {input.bam} -o {output.bed} \
        -summary {output.summary}  \
        -L {target} --BQSR {input.table} > {log} 2>&1''')
        if input_dict[wildcards.sample]['panel'] is None:
            db_targetname = input_dict[wildcards.sample]['pakket']
        else:
            db_targetname = input_dict[wildcards.sample]['panel']
        summary2db(wildcards.sample, read_summary(output.summary),
                   db_targetname, serie, METRICSDB)


rule getsangers:
    input:
        rules.callableloci.output.bed
    output:
        sangers = temp("gatkfiles/{sample}.sangers.txt"),
        tempbed = temp("gatkfiles/{sample}.tmp.bed")

    message:
        "Defining sangerfragments from callable regions"

    run:
        annot_bed = input_dict[wildcards.sample]['annot']
        panel = input_dict[wildcards.sample]['panel']
        if panel is not None and 'OVR' in panel:
            with open(output.sangers, 'w') as f:
                f.write('Geen sangers: OVR panel')
            shell("touch {output.tempbed}")
        else:
            df = annotate_callables(input, annot_bed, output.tempbed)
            sangercount = 0
            with open(output.sangers, 'w') as f:
                for target, data in df.groupby(df.index):
                    if len(data[data['callable'] != 'CALLABLE']) > 0:
                        sangercount += 1
                        gene = ' '.join(data['gene'].unique())
                        start = data[data['callable'] != 'CALLABLE']['regionstart'].min()
                        end = data[data['callable'] != 'CALLABLE']['regionend'].max()
                        f.write('{}\t{}\t{}\t{}\t{}\t{}\n'.format(
                            gene, target[0], start, end, target[1], target[2]))
                if sangercount == 0:
                    f.write('Geen sangers: alles callable')


rule depthofcoverage:
    input:
        bamfile = rules.markduplicates.output.bam,
        table = rules.recalibrate.output
    output:
        "gatkfiles/{sample}.DoC",
        temp("gatkfiles/{sample}.DoC.sample_cumulative_coverage_counts"),
        temp("gatkfiles/{sample}.DoC.sample_cumulative_coverage_proportions"),
        temp("gatkfiles/{sample}.DoC.sample_interval_statistics"),
        temp("gatkfiles/{sample}.DoC.sample_statistics"),
        temp("gatkfiles/{sample}.DoC.sample_summary"),
        "gatkfiles/{sample}.DoC.sample_interval_summary"
    message:
        "Calculating DepthOfCoverage with GATK"
    log:
        "logfiles/{sample}.DeptOfCoverage.log"
    run:
        target = input_dict[wildcards.sample]['cnvtarget']
        shell('''{JAVA} -jar {GATK} -R {REF} -T DepthOfCoverage \
        -I {input.bamfile} -o {output[0]} \
        -L {target} \
        --minBaseQuality 20 \
        --minMappingQuality 20 \
        --printBaseCounts  \
        -dels -BQSR {input.table} > {log} 2>&1
        ''')

        if input_dict[wildcards.sample]['capispakket']:
            perc_covered = calc_perc_target_covered(output[0])

        elif not input_dict[wildcards.sample]['capispakket']:
            pakket_target = input_dict[wildcards.sample]['pakkettarget']
            perc_covered = calc_perc_target_covered(output[0],
                                                    filter_regions=True,
                                                    target=pakket_target)
        pakket = input_dict[wildcards.sample]['pakket']
        perc_target_covered2db(wildcards.sample, perc_covered, pakket, serie, METRICSDB)


rule callvariants:
    input:
        bamfile = rules.markduplicates.output.bam,
        table = rules.recalibrate.output
    output:
        temp("gatkfiles/{sample}.raw.vcf"),
        temp("gatkfiles/{sample}.raw.vcf.idx")
    message:
        "Calling variants with GATK's HaplotypeCaller"
    log:
        "logfiles/{sample}.HaplotypeCaller.log"
    run:
        target = input_dict[wildcards.sample]['varcal']
        shell('''{JAVA} -jar {GATK} -R {REF} -T HaplotypeCaller \
        -I {input.bamfile} -o {output[0]} -L {target} -ip 500 \
        -pairHMM VECTOR_LOGLESS_CACHING -BQSR {input.table}  > {log} 2>&1
        ''')


rule filtervariants:
    input:
        vcf = rules.callvariants.output[0],
        index = rules.callvariants.output[1]
    output:
        "output/{sample}.filtered.vcf"
    log:
        "logfiles/{sample}.VariantFiltration.log"
    message:
        "Annotating variants with GATK's QC annotations"
    shell:
        '''{JAVA} -jar {GATK} -R {REF} -T VariantFiltration \
        -V {input.vcf} -o {output} \
        --clusterWindowSize 20 --clusterSize 6 \
        --filterExpression "DP < 30 " --filterName "LowCoverage" \
        --filterExpression "QUAL < 50.0 " --filterName "LowQual" \
        --filterExpression "QD < 4.0 " --filterName "QD" \
        --filterExpression "SOR > 10.0 " --filterName "SOR" > {log} 2>&1
        '''


rule callsnpcheck:
    input:
        rules.markduplicates.output.bam
    output:
        vcf = temp("gatkfiles/{sample}.snpcheck.vcf"),
        index = temp("gatkfiles/{sample}.snpcheck.vcf.idx")
    message:
        "Calling SNP checks sample"
    log:
        "logfiles/{sample}.CallSNPcheck.log"
    shell:
        '''{JAVA} -jar {GATK} -R {REF} -T HaplotypeCaller \
        -I {input} -o {output.vcf} -L {SNPCHECK} -alleles {SNPCHECK} \
        -gt_mode GENOTYPE_GIVEN_ALLELES -GQB 0 > {log} 2>&1
        '''


rule comparesnpcheckssample:
    input:
        vcf = rules.callsnpcheck.output.vcf,
        alt = "SNPcheck/{sample}.qpcrsnpcheck"
    output:
        ngs = temp("SNPcheck/{sample}.snpcheck.ngsextra.txt"),
        comp = temp("SNPcheck/{sample}.snpcheck.comp.temp")

    message:
        "Comparing SNP checks sample"
    log:
        "logfiles/{sample}.CompareSNPcheck.log"
    run:
        sd = parse_taqman(input.alt)
        ngsd = parse_ngssnpcheck(input.vcf)
        snpout = compare_snpchecks(sd, ngsd)
        rsid_to_locus = get_rs_gpos_dict()

        with open(output.comp, 'w') as f:
            for k, v in snpout.items():
                try:
                    ngsresult = ngsd[k]
                except KeyError:
                    ngsresult = 'NoNGS'

                out = ("{loc}\t{rs}\t{ngs}\t{sang}\t{result}\n"
                      .format(loc=k, rs=rsid_to_locus[k],
                              ngs=ngsresult, sang=sd[k],
                              result=v))
                f.write(out)

            if not snpout:
                for k, v in ngsd.items():
                    try:
                        rsid = rsid_to_locus[k]
                    except KeyError as e:
                        rsid = 'NGS-only'
                    else:
                        out = ("{loc}\t{rs}\t{ngs}\tNoTaqMan\tNoTaqMan\n"
                              .format(loc=k, rs=rsid,
                                      ngs=v)
                              )
                        f.write(out)


        with open(output.ngs, 'w') as f:
            [f.write('{loc}\tonbekend\t{ngs}\n'.format(loc=k, ngs=v))
             for k, v in ngsd.items() if k not in sd.keys()]


rule comparesnpchecksserie:
    input:
        expand(rules.comparesnpcheckssample.output.comp, sample=samples)
    output:
        temp("SNPcheck/duplicate.snpcheck.txt")
    message:
        "Comparing SNP checks serie"
    log:
        "logfiles/SNPcheckCompare.log"
    run:
        from collections import defaultdict
        df = pd.concat(
            [pd.read_csv(f, sep='\t', header=None, usecols=[0, 2],
             names=['locus', f.split('/')[-1].split('.')[0]],
             index_col='locus')
                for f in input],
            axis=1)
        # key = string of calls, value = dnr --> >1 value = double SNP check
        d = defaultdict(list)
        for col in df:
            x = str(df[col].values)
            d[x].append(col)

        with open(output[0], 'w') as f:
            for k, v in d.items():
                if len(v) > 1:
                    f.write(' '.join(v))


rule duplicatesnpcheck:
    input:
        serie = rules.comparesnpchecksserie.output,
        sample = rules.comparesnpcheckssample.output.comp,
        extrangs = rules.comparesnpcheckssample.output.ngs

    output:
        snpcomp = temp("SNPcheck/{sample}.snpcheck.comp.txt")

    message:
        "Creating SNPcheck output"
    log:
        "logfiles/SNPcheckCompare.log"
    run:
        import os
        os.rename(str(input.sample), str(output.snpcomp))
        with open(input.serie[0], 'r') as f:
            for line in f:
                line.strip()
                if wildcards.sample in line:
                    print_extra_ngscalls(output.snpcomp, input.extrangs)


rule addcnvdata:
    input:
        rules.depthofcoverage.output[-1]
    output:
        temp("gatkfiles/{sample}.cnv_data_added.txt")
    message:
        "Add CNV data"
    log:
        "logfiles/{sample}.CNV_add_data.log"
    run:
        if input_dict[wildcards.sample]['cnvscreening']:
            target = input_dict[wildcards.sample]['capture']
            shell("{pyCNV} --analyse -c {target} -s {serie} -n {input} --sample {wildcards.sample} --addonly > {log} 2>&1")
            shell("touch {output}")
        else:
            shell("touch {output}")


rule cnvdetection:
    input:
        expand(rules.addcnvdata.output, sample=samples)
    output:
        temp("gatkfiles/cnvdetection.txt")
    message:
        "CNV detection"
    log:
        "logfiles/CNV_detector.log"
    run:
        for capture in captures:
            screening = False
            for s in samples:
                workdir = os.path.join(os.getcwd(), 'output', capture, 'CNV')
                if input_dict[s]['capture'] == capture:
                    screening = input_dict[s]['cnvscreening']
            if screening:
                os.makedirs(workdir, exist_ok=True)
                shell("{pyCNV} --analyse -c {capture} -s {serie} -o {workdir} > {log} 2>&1")
        shell("touch {output}")


include:
    'docs/callmosaic.rules'

include:
    'docs/samplereport.rules'

include:
    'docs/seriereport.rules'

include:
    'docs/qcplots.rules'


rule END:
    input:
        expand(rules.filtervariants.output, sample=samples),
        expand(rules.samplereport.output, sample=samples),
        expand(rules.markduplicates.output.bam, sample=samples),
        rules.qcplots.output,
        rules.seriereport.output
